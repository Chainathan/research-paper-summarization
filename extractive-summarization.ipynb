{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1680a89899b137fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T03:32:35.860015Z",
     "start_time": "2024-04-14T03:32:35.857011Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget -O train.zip --no-check-certificate https://huggingface.co/datasets/ccdv/arxiv-summarization/resolve/main/train.zip?download=true\n",
    "# !Expand-Archive -Path \"train.zip\"\n",
    "# !move train.txt train.json\n",
    "# !del train.zip\n",
    "# import requests\n",
    "# from zipfile import ZipFile\n",
    "# import os\n",
    "\n",
    "# # Download the file\n",
    "# url = \"https://huggingface.co/datasets/ccdv/arxiv-summarization/resolve/main/train.zip?download=true\"\n",
    "# response = requests.get(url)\n",
    "# with open(\"train.zip\", \"wb\") as file:\n",
    "#     file.write(response.content)\n",
    "\n",
    "# # Unzip the file\n",
    "# with ZipFile('train.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('.')\n",
    "\n",
    "# # Move and rename the file (adjust the filenames if necessary)\n",
    "# os.rename('train.txt', 'train.json')  # Make sure the source filename is correct\n",
    "\n",
    "# # Delete the zip file\n",
    "# os.remove('train.zip')\n",
    "\n",
    "# !powershell -Command \"Invoke-WebRequest -Uri 'https://huggingface.co/datasets/ccdv/arxiv-summarization/resolve/main/train.zip?download=true' -OutFile 'train.zip'\"\n",
    "# !powershell -Command \"Expand-Archive -Path 'train.zip' -DestinationPath '.'\"\n",
    "# !move train.txt train.json\n",
    "# !del train.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d74c6dd6ff94a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # load papers in json format from each line of the extracted file\n",
    "# import json\n",
    "# papers = []\n",
    "# article_ids = []\n",
    "# f = 1\n",
    "# with open('test.txt', 'r') as file:\n",
    "#     for line in file:\n",
    "#         try:\n",
    "#             # Parse the JSON data from each line\n",
    "#             paper_data = json.loads(line)\n",
    "#             if f == 1:\n",
    "#               print(\"paper_data: \", paper_data)\n",
    "#               f += 1\n",
    "#             # Extract the arXiv ID and append to the list\n",
    "#             if 'article_id' in paper_data:\n",
    "#                 article_ids.append(paper_data['article_id'])\n",
    "#             papers.append(paper_data)\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             # Output an error message if a line is not valid JSON\n",
    "#             print(f\"Error parsing JSON for line: {line}\")\n",
    "#             print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f105075c16eefb63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Print stuff\n",
    "# json_formatted_str = json.dumps(papers[1][\"sections\"][1], indent=2)\n",
    "# print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712066a0c00dedf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LSA Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ed3daf4aa4e7c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stop-words\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # print(\"tokens: \", tokens)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # print(\"filtered tokens: \", filtered_tokens)\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    #print(\"doc after joining: \", doc)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ed999a81124316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:28:34.871869Z",
     "start_time": "2024-04-15T01:28:34.869549Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c969ac557b490a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:28:35.272046Z",
     "start_time": "2024-04-15T01:28:34.872633Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "# Get top n sentences. Default value of n is 4.\n",
    "def get_sentences_top(sentences, num_sentences=4):\n",
    "    norm_sentences = normalize_corpus(sentences)\n",
    "    norm_sentences = list(filter(None, norm_sentences))\n",
    "    #print(\"norm sentences: \", norm_sentences)\n",
    "    tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "    if len(norm_sentences) == 0:\n",
    "        return np.array([])\n",
    "    dt_matrix = tv.fit_transform(norm_sentences)\n",
    "    #print(\"dt matrix: \", dt_matrix)\n",
    "    dt_matrix = dt_matrix.toarray()\n",
    "    #print(\"dt matrix after converting it to array: \", dt_matrix)\n",
    "    vocab = tv.get_feature_names_out()\n",
    "    # print(\"Vocab: \", vocab)\n",
    "    td_matrix = dt_matrix.T\n",
    "    #print(td_matrix.shape)\n",
    "    pd.DataFrame(np.round(td_matrix, 2), index=vocab).head(10)\n",
    "    num_sentences = min(num_sentences, len(sentences))\n",
    "    #print(\"num_sentences\", num_sentences)\n",
    "    num_topics = 3\n",
    "    if(len(norm_sentences) <= num_topics):\n",
    "        return np.array(sentences)\n",
    "\n",
    "    u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\n",
    "    #print(u.shape, s.shape, vt.shape)\n",
    "    term_topic_mat, singular_values, topic_document_mat = u, s, vt\n",
    "\n",
    "    sv_threshold = 0.5\n",
    "    min_sigma_value = max(singular_values) * sv_threshold\n",
    "    singular_values[singular_values < min_sigma_value] = 0\n",
    "    salience_scores = np.sqrt(np.dot(np.square(singular_values),\n",
    "                                 np.square(topic_document_mat)))\n",
    "    top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
    "    top_sentence_indices.sort()\n",
    "    return np.array(sentences)[top_sentence_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba06340fce3c7fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:28:35.275570Z",
     "start_time": "2024-04-15T01:28:35.272843Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a method to perform extractive summarization)\n",
    "def extractive(document_sections, num_sentences=4):\n",
    "    overall_summary = \"\"\n",
    "    for section in document_sections:\n",
    "        summary = get_sentences_top(section, num_sentences)\n",
    "        overall_summary += \" \".join(summary) + \" \"\n",
    "    return overall_summary\n",
    "\n",
    "# combine sentences in abstract and remove the <s> and </s> tokens at the beginning and end of the sentence\n",
    "def abstract_combiner(abstract):\n",
    "    combined = \"\"\n",
    "    for sentence in abstract:\n",
    "        combined += (sentence[4:-4]) + \" \"\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1759f3bd5022f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:14:53.604920Z",
     "start_time": "2024-04-15T01:14:53.521772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f18e6d9ff5660f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T03:32:39.681363Z",
     "start_time": "2024-04-14T03:32:39.678545Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# abstract_combiner(papers[0][\"abstract_text\"])\n",
    "# extractive(papers[0]['sections'], num_sentences=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2113490677241a45",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CSV dataset\n",
    "# import csv\n",
    "# # Define the header and data separately\n",
    "# header = [\"instruction\", \"input\", \"output\"]\n",
    "# \n",
    "# # Path to save the CSV file\n",
    "# file_path = 'sample-data.csv'\n",
    "# instruction = \"Summarize\"\n",
    "# # Open the file in write mode\n",
    "# with open(file_path, 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     # Write the header\n",
    "#     writer.writerow(header)\n",
    "#     # Write the data rows\n",
    "#     for row in papers[:10]: # Do for sample 10 papers\n",
    "#         input = extractive(row['sections'], num_sentences=4)\n",
    "#         output = abstract_combiner(row[\"abstract_text\"])\n",
    "#         writer.writerow([row['article_id'], instruction, input, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970bbb72ed3b3d5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # JSON for small data (Load entire dataset in-memory)\n",
    "# from tqdm import tqdm\n",
    "# json_data = []\n",
    "# \n",
    "# instruction = \"Produce a clear and concise abstract summarizing the key points from the input, emphasizing the objectives, methodology, results, and conclusions without detailed technical jargon or formulas. Ignore @xmath terms.\"\n",
    "# for row in tqdm(papers[:1000]): # Do for sample 10 papers\n",
    "#     input = extractive(row['sections'], num_sentences=5)\n",
    "#     output = abstract_combiner(row[\"abstract_text\"])\n",
    "#     article_id = row[\"article_id\"]\n",
    "#     section_names = row[\"section_names\"]\n",
    "#     json_data.append({\n",
    "#         \"article_id\": article_id, \n",
    "#         \"instruction\": instruction, \n",
    "#         \"input\": input, \n",
    "#         \"output\": output,\n",
    "#         \"section_names\": section_names})\n",
    "#     \n",
    "# # Path to save the JSON file\n",
    "# file_path = 'sample_data.json'\n",
    "# with open(file_path, 'w') as file:\n",
    "#     # json.dump(json_data, file, separators=(',', ':'))\n",
    "#     json.dump(json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425d95d71d010a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:28:44.812763Z",
     "start_time": "2024-04-15T01:28:44.806446Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convenience method\n",
    "instruction = \"Produce a clear and concise abstract summarizing the key points from the input, emphasizing the objectives, methodology, results, and conclusions without detailed technical jargon or formulas. Ignore @xmath terms.\"\n",
    "def preprocess_function(paper):\n",
    "    input = extractive(paper['sections'], num_sentences=5)\n",
    "    output = abstract_combiner(paper[\"abstract_text\"])\n",
    "    return {\n",
    "    \"article_id\": paper[\"article_id\"], \n",
    "    \"instruction\": instruction, \n",
    "    \"input\": input, \n",
    "    \"output\": output,\n",
    "    \"section_names\": paper[\"section_names\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79153709402b4f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T01:28:45.364484Z",
     "start_time": "2024-04-15T01:28:45.342307Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Modularized method for small JSON data (Still In-memory)\n",
    "# file_path = 'test_data.json'\n",
    "# with open(file_path, 'w') as file:\n",
    "#     file.write('[')\n",
    "#     for i, item in enumerate(tqdm(papers[:10], desc=\"Processing items\")):\n",
    "#         processed_item = preprocess_function(item)\n",
    "#         json_string = json.dumps(processed_item, separators=(',', ':'))\n",
    "#         if i != 0:\n",
    "#             file.write(',')\n",
    "#         file.write(json_string)  \n",
    "#     file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1911372469db600",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 1036it [01:49, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in extractive summarization for: 1608.03781\n",
      "empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 1237it [02:12,  8.15it/s]"
     ]
    }
   ],
   "source": [
    "# Sequential Disk-to-Disk implementation \n",
    "# Note: Multi-processing seems to be slowing things down.\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "in_file_path = 'train.json'\n",
    "out_file_path = 'train_data.json'\n",
    "\n",
    "with open(in_file_path, 'r') as infile:\n",
    "    with open(out_file_path, 'w') as outfile:\n",
    "        outfile.write('[')\n",
    "        for i, line in enumerate(tqdm(infile, desc=\"Processing items\")):\n",
    "            try:\n",
    "                # Load, process, and serialize line to JSON string\n",
    "                paper_data = json.loads(line) \n",
    "                processed_item = preprocess_function(paper_data)\n",
    "                json_string = json.dumps(processed_item, separators=(',', ':'))\n",
    "                if i != 0:\n",
    "                    outfile.write(',')\n",
    "                outfile.write(json_string)  \n",
    "            except json.JSONDecodeError as e:\n",
    "                # Output an error message if a line is not valid JSON\n",
    "                print(f\"Error parsing JSON for line: {line}\")\n",
    "                print(str(e))\n",
    "            except ValueError as e:\n",
    "                print(f\"Error in extractive summarization for: {paper_data['article_id']}\")\n",
    "                print(e)\n",
    "        outfile.write(']')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
